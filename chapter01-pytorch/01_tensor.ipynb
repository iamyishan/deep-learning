{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771d22b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本: 2.5.0+cu124\n",
      "CUDA 是否可用: True\n",
      "可用的 CUDA 设备数量: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch 版本:\", torch.__version__)\n",
    "print(\"CUDA 是否可用:\", torch.cuda.is_available())\n",
    "print(\"可用的 CUDA 设备数量:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a984bdc",
   "metadata": {},
   "source": [
    "### 1.Tensor基本概念\n",
    "Tensor可以说是PyTorch里最重要的概念，PyTorch把对数据的存储和操作都封装在Tensor里。PyTorch里的模型训练的输入输出数据，模型的参数，都是用Tensor来表示的。Tensor在操作方面和NumPy的ndarray是非常类似的。不同的是Tensor还实现了像GPU计算加速，自动求导等PyTorch的核心功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97eb94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1D Tensor\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "print(t1)\n",
    "\n",
    "# 2D Tensor\n",
    "t2 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(t2)\n",
    "\n",
    "# 3D Tensor\n",
    "t3 = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "print(t3)\n",
    "\n",
    "# 从 NumPy 创建 Tensor\n",
    "arr = np.array([1, 2, 3])\n",
    "t_np = torch.tensor(arr)\n",
    "print(t_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f438b",
   "metadata": {},
   "source": [
    "在创建tensor时，PyTorch会根据你传入的数据，自动推断tensor的类型，当然，你也可以自己指定类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebf0531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor((2,2),dtype=torch.float32)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac964cc",
   "metadata": {},
   "source": [
    "PyTorch里的数据类型，主要为：\n",
    "\n",
    "整数型 torch.uint8、torch.int32、torch.int64。其中torch.int64为默认的整数类型。\n",
    "\n",
    "浮点型 torch.float16、torch.bfloat16、 torch.float32、torch.float64，其中torch.float32为默认的浮点数据类型。\n",
    "\n",
    "布尔型 torch.bool\n",
    "\n",
    "在PyTorch里使用最广泛的就是浮点型tensor。其中torch.float32称为全精度，torch.float16/torch.bfloat16称为半精度。一般情况下模型的训练是在全精度下进行的。如果采用混合精度训练的话，会在某些计算过程中采用半精度计算。混合精度计算会节省显存占用以及提升训练速度。\n",
    "\n",
    "PyTorch里没有字符串类型，因为Tensor主要关注于数值计算，并不需要支持字符串类型。\n",
    "\n",
    "Bool类型在PyTorch里可以进行高效的索引选择，所以PyTorch支持Bool类型。比如Bool类型tensor进行索引操作示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925688d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False,  True,  True,  True])\n",
      "tensor([3, 4, 5])\n",
      "tensor([1, 2, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "mask = x > 2  # 生成一个布尔掩码\n",
    "print(mask)   # tensor([False, False,  True,  True,  True])\n",
    "\n",
    "# 用布尔掩码选出大于 2 的值\n",
    "filtered_x = x[mask]\n",
    "print(filtered_x)  # tensor([3, 4, 5])\n",
    "\n",
    "\n",
    "# 用布尔掩码选出大于 2 的值,并赋值为0\n",
    "x[mask]=0\n",
    "print(x) # tensor([1, 2, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87f7cc",
   "metadata": {},
   "source": [
    "在创建tensor时，你还可以指定tensor的设备。如果你不指定，默认是在CPU/内存上。如果你想创建一个GPU/显存上的tensor。可以通过把device关键字设定为“cuda”来指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca4a0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_gpu = torch.tensor([1,2,3],device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ca6ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_tensor: tensor([[0.3258, 0.9423, 0.8552],\n",
      "        [0.2149, 0.4189, 0.7921]])\n",
      "randn_tensor: tensor([[ 0.0417, -0.8296,  0.3751],\n",
      "        [ 1.0237,  1.8611,  2.1555]])\n",
      "ones_tensor: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "zeros_tensor: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "twos_tensor: tensor([[2, 2, 2],\n",
      "        [2, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape) # 生成一个从[0,1]均匀抽样的tensor。\n",
    "randn_tensor = torch.randn(shape) # 生成一个从标准正态分布抽样的tensor。\n",
    "ones_tensor = torch.ones(shape) #生成一个值全为1的tensor。\n",
    "zeros_tensor = torch.zeros(shape) # 生成一个值全为0的tensor。\n",
    "twos_tensor = torch.full(shape, 2) #  生成一个值全为2的tensor。\n",
    "\n",
    "print(\"rand_tensor:\",rand_tensor)\n",
    "print(\"randn_tensor:\",randn_tensor)\n",
    "print(\"ones_tensor:\",ones_tensor)\n",
    "print(\"zeros_tensor:\",zeros_tensor)\n",
    "print(\"twos_tensor:\",twos_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1a658",
   "metadata": {},
   "source": [
    "### 2.Tensor的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1062b4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "requires_grad: \n",
      " False\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "print(f\"requires_grad: \\n {tensor.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a71bf",
   "metadata": {},
   "source": [
    "### 3.Tensor的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0ae0c",
   "metadata": {},
   "source": [
    "形状变换\n",
    "\n",
    "在对Tensor进行操作的过程中，我们经常要对tensor进行形状的改变。这里我们介绍常用的一些操作。\n",
    "\n",
    "x = torch.randn(4,4) #  生成一个形状为4x4的随机矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16c433ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1638, -0.9537,  1.6809, -2.0100, -1.5777, -0.9907, -0.1635,  0.9980],\n",
      "        [-0.4567,  0.1301,  0.0367,  0.8249,  1.3055,  1.0466,  1.1066, -0.4691]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,4) #  生成一个形状为4x4的随机矩阵。\n",
    "x = x.reshape(2,8) # 通过reshape操作，可以将4x4的矩阵改变为2x8的矩阵。\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83097ad4",
   "metadata": {},
   "source": [
    "上边通过reshape操作将4x4的矩阵改变为2x8的矩阵。你也可以将这个矩阵改为1x16的矩阵，只要元素个数一致就可以。\n",
    "\n",
    "你可以通过permute函数来交换tensor的维度（转置），需要注意的是它的作用与reshape不同，reshape是按元素顺序重新组织维度，permute会改变元素的顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8adfe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([2, 3])\n",
      "reshape: tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "permute: tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"shape:\",x.shape)\n",
    "x_reshape = x.reshape(3,2)\n",
    "x_transpose = x.permute(1,0) #交换第0个和第1个维度。对于二维矩阵就是行列互换，进行转置。\n",
    "print(\"reshape:\",x_reshape)\n",
    "print(\"permute:\",x_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d22f6",
   "metadata": {},
   "source": [
    "对于二维tensor，你可以调用tensor.t()方法进行转置操作。 有时，需要扩展tensor的维度，可以使用unsqueeze函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6577485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3]) tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]]])\n",
      "torch.Size([2, 1, 3]) tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]])\n",
      "torch.Size([2, 3, 1]) tensor([[[1],\n",
      "         [2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5],\n",
      "         [6]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "#扩展第0维\n",
    "x_0 = x.unsqueeze(0)\n",
    "print(x_0.shape,x_0)\n",
    "#扩展第1维\n",
    "x_1 = x.unsqueeze(1)\n",
    "print(x_1.shape,x_1)\n",
    "#扩展第2维\n",
    "x_2 = x.unsqueeze(2)\n",
    "print(x_2.shape,x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedc5f8",
   "metadata": {},
   "source": [
    "你可以使用tensor的squeeze方法来缩减tensor的大小为1的维度。你可以指定需要缩减的维度索引，如果不指定，则会缩减所有大小为1的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75b6368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3]) tensor([[[1., 1., 1.]]])\n",
      "torch.Size([1, 3]) tensor([[1., 1., 1.]])\n",
      "torch.Size([3]) tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1,1,3))\n",
    "print(x.shape, x)\n",
    "y = x.squeeze(dim=0)\n",
    "print(y.shape, y)\n",
    "z = x.squeeze()\n",
    "print(z.shape, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98b2a3",
   "metadata": {},
   "source": [
    "#### 3.1数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04002ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,3))\n",
    "b = torch.ones((2,3))\n",
    "\n",
    "print(a + b)  # 加法\n",
    "print(a - b)  # 减法\n",
    "print(a * b)  # 逐元素乘法\n",
    "print(a / b)  # 逐元素除法\n",
    "print(a @ b.t())  # 矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a8016",
   "metadata": {},
   "source": [
    "#### 3.2统计函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bacd7",
   "metadata": {},
   "source": [
    "一个tensor中包含多个元素，对这些元素可以进行统计操作。比如通过tensor.sum()求和，通过tensor.mean()求均值，通过tensor.std()求标准差，通过tensor.min()求最小值等。\n",
    "\n",
    "以计算均值为例，对于一个3x2的tensor，我们可以整体求均值，也可以统计每一行的均值，或者每一列的均值。\n",
    "\n",
    "Tensor指定统计的维度，意味着要“消灭”这个维度。比如指定dim=0意味着，统计结果要“消灭”行这个维度，各个列的值在不同行上进行统计。\n",
    "\n",
    "如果你想让tensor对某一维度进行统计后，保持原来的维度不变，不会“消灭”统计的维度。你可以指定参数keepdim=True。t1.mean(dim=0,keepdim=True)结果的shape为(1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c5bf999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor(2.)\n",
      "mean on dim 0: tensor([1., 3.])\n",
      "keepdim: tensor([[1., 3.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([[1.0, 3.0], [1.0, 3.0], [1.0, 3.0]])\n",
    "\n",
    "mean = t.mean()\n",
    "print(\"mean:\", mean)\n",
    "\n",
    "mean = t.mean(dim=0)\n",
    "print(\"mean on dim 0:\", mean)\n",
    "\n",
    "mean = t.mean(dim=0, keepdim=True)\n",
    "print(\"keepdim:\", mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904091a1",
   "metadata": {},
   "source": [
    "#### 3.3索引和切片\n",
    "和python里的序列数据类似，tensor也支持索引和切片操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a7d0bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "tensor([2, 5])\n",
      "tensor([4, 5, 6])\n",
      "tensor([[1, 2],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x[0, 1])  # 访问第一行第二个元素\n",
    "print(x[:, 1])  # 访问第二列\n",
    "print(x[1, :])  # 访问第二行\n",
    "print(x[:, :2])  # 访问前两列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cd203",
   "metadata": {},
   "source": [
    "#### 3.4 广播机制\n",
    "\n",
    "原则上来说，tensor的所有的逐元素运算都要求两个tensor的形状必须完全一致。比如对于tensorA和tensorB进行逐元素计算，只有tensorA的形状与tensorB的形状完全一致。才能保证tensorA的每个元素都有与之对应的tensorB的元素来进行计算。\n",
    "\n",
    "但在实际中，假如我们有一个tensor：t1。t1的shape为（3，2）。我们想给t1的每个元素都加上1。此时我们不必构造一个shape为（3,2），元素全为1的tensor再进行相加。我们可以直接写 t1 +1，PyTorch内部会虚拟扩展出一个形状为（3,2）的tensor，再和t1相加。这种机制，就是广播机制。需要注意的是，PyTorch 在进行广播计算时，并不会真的复制数据，而是通过调整张量的索引方式（Strided Memory Access）来实现逐元素计算。从而节省大量的存储，提高计算效率。示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "036b3928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8741, -0.4695],\n",
      "        [ 0.8999,  0.5185],\n",
      "        [-0.8849,  0.6084]])\n",
      "tensor([[1.8741, 0.5305],\n",
      "        [1.8999, 1.5185],\n",
      "        [0.1151, 1.6084]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn((3,2))\n",
    "print(t1)\n",
    "t2 = t1 + 1 # 广播机制\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad9cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
